# IO机制

一篇非常有参考价值的CSDN博文：[链接](https://blog.csdn.net/qq_37555071/article/details/113932533)

## 一次IO经历了什么

我们知道，Linux把所有东西都视为文件，不同类型的文件有不同的特殊方法。对于网络IO而言，套接字就是文件接口，可以对这个接口进行读写；对于磁盘IO而言，读写就是对扇区的访问。

### DMA

IO过程影响到两个东西，一个是CPU，一个是进程。早期的CPU在遇到IO过程的时候，会触发特殊的中断，然后用大量的时间来完成数据的传输；CPU在IO期间没有办法完成其他操作，相当于CPU阻塞了，在为一个进程服务。

为了在IO的时候节约CPU时间，就有了DMA，只需要CPU发起一次调用，DMA就能完成后续的传输，在传输完成后触发特殊的系统中断。使用了DMA之后，一个进程发起一次I/O请求之后，CPU并不会被阻塞，但是进程肯定是被阻塞了。

### 硬盘读写

因为硬盘访问比较慢，远远跟不上CPU的速度，但是快速缓存的成本和继承难度都很高。现在计算机的存储机制是多级缓存+内存+硬盘。内存中加载了最近需要访问的数据，缓存中又存储了一部分内存最可能用到的数据。

不考虑缓存，我们先考虑最简单的文件读写过程：

当应用程序要读取一个文件内容的时候，先在内核缓冲空间找数据，如果找不到则从硬盘发起一次DMA读取到内核缓冲区；然后从内核缓冲区有一次拷贝到用户（应用）缓存区。

当应用程序要写一个文件内容的时候，也是从用户缓冲区拷贝到内核缓冲区；实际落盘的时间，如何保证数据落盘，都是操作系统决定的（也可以显式指定），但是最终会从内核缓冲区落盘。因此读和写都是两次拷贝。但是如果写的时候数据没有准备好，则需要额外两次读取的拷贝。

### 网络IO

网络IO和磁盘IO的逻辑是一样的，我们会发现以下的异同：

1. 网络Output什么时候实际完成也是由系统决定的，也可以由应用程序强制发送。但是如果对端设备没准备好，会有长时间的进程阻塞，和磁盘IO不太一样。磁盘IO的写阻塞对于上层应用而言一般是不透明的，由文件系统来具体解决。
2. 网络Input也可能会有长时间的进程阻塞，主要是等待对方的输入以及网络时延；硬盘读取也会有阻塞，但是是因为磁盘响应时延，通常在10ms（毕竟是硬件磁头），但是这个时延也就相当于网络传输时延，不会因为磁盘没准备好而等待很长的时间。
3. 网络IO也可以使用DMA（想不到吧）网卡设备也是硬件设备，也可以和内核缓冲区直接通过DMA进行传输，在传输完成之后给CPU发一个中断信号即可。

总的来说，相比于磁盘IO，网络IO等待数据的阻塞时间更长。在获得数据之后，网络IO也要将数据从内核空间拷贝到用户空间。

### 小结

不管是网络IO还是磁盘IO，首先系统会尝试转为缓冲区IO。如果需要读/写的数据就在缓冲区中，则可以减少很多步骤。以数据读取为例，如果缓冲区有数据，则可以直接从缓冲区读取；否则需要经过两次拷贝，从设备接受到内核缓冲区，再拷贝到用户缓冲区再读取。

硬盘数据读取主要耗时是等待磁头就位，大约需要10ms，耗时相当长；硬盘数据写入则由文件系统控制脏页落盘，对于正常的上层应用而言是没有写入的耗时的。

网络数据读取主要耗时是等待网络数据就位，往往会比硬盘数据时间长的多；当有数据到达的时候，会触发系统的硬件中断，传输到内核缓冲区；数据在内核缓冲区就位后，还需要拷贝到用户缓冲区。

DMA解决了数据传输到内核缓冲区这一过程对CPU的占用，但是等待数据到达还是不可避免的需要等待。网络IO可以理解为两段阻塞，一段是等待数据到达与就位，时间长；一段是数据拷贝到用户缓冲区，时间短。

## 五种IO策略

前文说到网络IO的特点：数据到达内核缓冲区的阻塞时间长。而随着互联网的发展，网络IO有了新的特点：并发数目巨大。随着并发数目的上升，我们发展出了五种IO策略。

### BIO

BIO，即阻塞IO（Blocking IO）。阻塞IO首先是同步IO，在接受网络数据的时候有阻塞。相对于其他IO策略，BIO在第一阶段等待数据到达内核缓冲区是阻塞的。

在使用DMA解决了数据到达内核缓冲区过程对CPU的占用之后，一台机器就可以处理很多的网络连接了。一个网络连接在服务器上对应一个进程/线程，这个进程在调用`recv`方法接收特定`socket`数据之后，就进入一个系统调用，进程进入阻塞状态。

进程进入阻塞阶段之后，就不再参与时间片竞争，也就是不占用CPU资源。这个时候进程会被放到对应`socket`的等待队列中。当数据到达设备之后，会触发硬件终端通过DMA传递给内核缓冲区，这个过程完全不需要进程和关心；数据到达`socket`对应的内核缓冲区之后，会给内核一个特殊的中断，内核会将数据拷贝到进程对应的用户缓冲区，然后唤醒等待队列中的进程，使其处于就绪状态，以执行后续的逻辑。

可以看到，在进程通过系统调用获取`socket`对应的数据之后，就会持续陷入阻塞状态，直到数据被拷贝到用户空间。

进程的阻塞在计算机并发调度里并不是什么可怕的事情。一个阻塞的进程在被唤起之前是不会参与CPU时间片轮转的竞争的。因此，如果是有大量进程同时工作，但是大多数时候这些进程都是在阻塞状态，将任务转交给内核。而内核也没有把大量的时间花费在IO的等待上，而是交给DMA和`socket`就绪后的系统中断。在网络IO中，绝大多数进程都会阻塞在等待输入上，也就意味着少量的CPU资源就能负担起大量的网络请求。

### NIO

NIO是非阻塞IO，Non-Blocking IO。注意和Java中的NIO并不是一个意思，Java中的NIO是New IO，实际上指代的是Async IO。

#### 阻塞IO遇到的问题

前面说了网络IO并发数高（业务特点决定）同时IO等待时间又长（网络IO的特点决定）。IO等待阻塞CPU，我们通过DMA解决了；配套使用的BIO也可以保证同时只有少数进程处于就绪状态参与时间片竞争。那么会有什么问题呢？

当并发数足够大的时候，就会出现问题。一个进程需要分配大约1MB的栈空间，而栈空间的大小是有限制的，例如4G就只能分配4k个进程（还不考虑内核空间）。使用线程虽然会引入一些线程安全的问题，但是可以显著缩小占用的空间。一个线程初始大小是1k，实际大小一般会在几十k到几百k之间。

而当我们使用BIO的时候，一个进程/线程只能监听一个套接字。如果使用BIO，且一个进程监听多个套接字，会出现这样的情况：

```python
# 假设监听socketA socketB
# sockets.recv() 不可行，没有这个方法
while True:
  socketA.recv()	# 执行完这句就直接阻塞了
  socketB.recv()  # 上一句阻塞的情况下，想运行这句只能多线程
# 多线程伪代码
# Thread(socketA.recv()).start()
# Thread(socketA.recv()).start()
```

可以看到，BIO要监听多个socket就必须使用同等数量的多进程/线程。

现在我们的矛盾是一个进程只能监听一个套接字，套接字数量非常庞大，而激活数量又很少。为了能实时处理这些套接字，需要的大量进程会耗尽系统资源。

#### NIO的解决方案

BIO遇到的问题是大量的网络请求需要大量的进程，那么反过来最直接的想法就是一个进程处理完所有网络请求。

进程通过`recv`这个系统调用会阻塞在数据准备，而且其中大量的时间都是用在等待数据到来，这个等待是强制的。那么最简单的思路就是把“等待数据到来”这件事的权力下放给进程本身。

假如我们对`socket`进行简单的改造，调用`recv`方法的时候不会阻塞，而是返回当前有没有数据。这样每个`socket`在`recv`之后可以立刻返回进程，也就是说进程并不会阻塞。

之后进程如果不想等待，就可以处理其他逻辑。或者进程就是想要等待这个`socket`，就再次询问即可。

例如：

```python
while True:
  for socket in sockets:
    if socket.recv():
      # do something
    else:
      continue # check others
```

NIO使用这个方法让一个进程就可以监听多个`socket`，省去了大量创建进程的开销。

### I/O multiplexing

#### 非阻塞IO的缺陷

为了进程阻塞在数据等待的问题，我们修改了socket允许其以NIO需要的方式返回“是否就绪”的状态，这样将等待的权利下放给进程自身。

NIO解决了进程阻塞在等待套接字数据的资源消耗，但是相应的，现在我们有一个不断轮询每个套接字是否就绪的大型进程，这个进程几乎不需要休眠，可以占满所有CPU资源。

在我们使用BIO的时候，没有数据到达的进程是处于阻塞状态的，不参与时间片竞争，也就不消耗CPU资源。而使用NIO需要一直循环监听。当`socket`数目增大的时候，光是遍历一次`socket`就需要非常多的时间，因此带来的CPU资源浪费和时延都是难以接受的。

#### 多路复用的原理

假如我们有10000个socket要监听，使用BIO会耗尽内存资源，使用NIO则轮询一次就要很漫长的时间，并且在只有少数socket活跃的时候依然会耗尽CPU资源。

I/O多路复用就是为了解决这个问题提出来的。

首先理解一下多路复用，这是一个通信技术，我们常用的TDM和FDM就是典型的多路复用，有很多业务要在一组物理资源上运行，就需要“复用”信道，然后通过一些方式转接。多路复用某种意义上就是一对多路开关，形如：

```python
# 多个TX | 当前Tx1  | 对应Rx1  | 多个Rx
#   Tx1 ----\          /---- Rx1
#   Tx2 ---- \.------./ ---- Rx2
#   Tx3 ----/          \---- Rx3
# 中间这条物理信道被多路信号复用(multiplexing)
```

IO多路复用也是如此，从字面意义上看就是为了“让多个socket复用一个进程资源”。IO多路复用和非阻塞IO要解决的问题是一样的，NIO只是修改`socket`方法是否阻塞，这种模型不够好，性能差，因此有了IO多路复用。

可以说，NIO是IO多路复用的最基础思路，而我们现在说的IO多路复用则是对NIO的改进。

#### `select`

NIO需要循环检查每个`socket`是不是处于就绪状态，因为这个检查没有内核空间的配合，所以效率低下。

我们知道，当我们的进程阻塞在一个`socket`的接受过程的时候，实际上就是将线程设为阻塞状态，然后放到这个`socket`的等待队列。那么如果有一种方法可以把进程同时添加到多个socket的等待队列中，自然就可以做到对多个socket的监听了。

当我们的进程阻塞在一个socket的接受过程的时候，实际上就是将线程设为阻塞状态，然后放到这个socket的等待队列。那么如果有一种方法可以把进程同时添加到多个socket的等待队列中，自然就可以做到对多个socket的监听了。

`select`就是用这个思路完成的单进程对多*sockets* 的监听。操作系统需要提供一个方法让进程可以把自己添加到多个socket的等待队列，在有socket就绪之后就把进程从这些socket的等待队列里都移除。

进程在调用`select`方法的时候，将要监听的*sockets* 列表传递给内核。之后，内核将这个进程添加到所有监听的sockets的等待队列中，进程进入阻塞状态。

有 *socket* 就绪之后，内核负责进程从这些 *sockets* 的等待队列里都移除，唤醒进程。当进程重新恢复到`select`调用的位置，内核遍历所有 *sockets* 来获得有多少已经就绪的，作为`select`的返回值。

进程离开`select`的阻塞状态之后，再次遍历所有 *sockets* ，根据实际情况决定接下来的具体逻辑，将数据拷贝到用户缓冲区。 

对比NIO，`select`方法虽然也有几次遍历操作，但是不需要再持续轮询占用CPU资源，所有任务都由内核接管也大大减少了用户态到内核态切换的开销。

#### `poll`

`select`方法提供了一个特殊的系统调用，用于管理进程同时阻塞在多个网络IO等待的情况。在管理过程中，`select`方法需要直接传入监听 *sockets* 列表，并且需要好几轮遍历。考虑到频繁拷贝的问题，`select`方法要求一次监听的 *socket* 数目不得超过1024。

poll是轮询的意思，`poll`和`select`本质上并没有区别，只是将传给内存的`fd_set`改成了链表结构，形式上取消了1024的监听限制，传参更加友好。性能上并没有什么变化。

#### `epoll`

所谓的`epoll`的`e`是event，也就是所谓的事件驱动。

在`select/poll`使用的过程中，我们确实成功让一个进程能够监听多个socket并实时做出响应，同时又解决了NIO方法中不断轮询对CPU资源的浪费（和耗时）。

但是，这种做法也遇到了如下问题：

1. 每次调用`select`都要带上参数`fd_set`，当监听的socket足够多的时候，传参开销也会变得非常大。
2. `select`接收到数据后唤起进程，进程只知道有socket就绪，具体是谁，进程还需要一次额外的遍历。
3. 多线程的情况下内核无法保证`select/poll`的状态。

要解决这三个问题也非常简单，`epoll`使用了如下的方式：

- 考虑到要监听的sockets变化较少，`epoll`将确定和修改sockets这一步同阻塞进程分离开。原来的`select()`被分为`epoll_ctl`和`epoll_wait`，大大减少了拷贝的开销。
- `sockets`现在是内核为一个`eventpoll`单独维护的结构中的一部分，通过`epoll_ctl`可以进行增删，为了维持查找和修改的性能，使用红黑树作为底层实现。红黑树是一种自平衡的二叉树，是2-3树的变体。
- `evetpoll`还维护一个`rdlist`结构，引用所有已经就绪的`socket`。这个设计使得进程只需要扫描`rdlist`就可以知道哪些`socket`已经就绪；每次阻塞之前也只要检查`rdlist`是否为空就知道有没有已经就绪的`socket`，不需要遍历。
- 通过在内核空间维护`eventpoll`，`epoll`现在是线程安全的了。
- `epoll`可以应用`mmap`技术来减少用户态和内核态之间的拷贝。mmap技术并不是一定要应用的，也不是`epoll`独享的，但是这个设计显然参考了`select`遇到的性能瓶颈，避免了扫描`rdlist`时候的开销。

`epoll`对多socket监听模式进行了很好的抽象，因此其性能是显著优于`select/poll`的。socket的变化并不会直接反映到等待队列，而是修改`eventpoll`，这一层抽象是`epoll`性能好的根本原因。

#### `epoll`的细节

`epoll`有两种触发方式，有的时候会问到。

常规的翻译，这两种方式是“水平触发”和“垂直触发”。当然，这个翻译并不好。

英文原本的说法是`level-triggered`和`edge-triggered`，其实按照通信逻辑去理解是非常简单的。`level`是电平，`level-triggered`就是只要有数据，这个socket就会持续触发事件提醒；`edge`是边沿，电平的“上升沿”/“下降沿”，`edge-triggered`就是当socket有数据变化，会触发一次提醒，后续并不会触发。

理解了原本的意义，这里翻译成“水平”肯定就不够精确，后者翻译为“垂直”有点意译的意思在里面，有些地方翻译成“边缘”肯定是不对的，翻译成“边沿”才算有点通信术语的感觉。

### 消息驱动IO

消息驱动IO和事件驱动有点相似，但是这个“信号”的含义非常狭窄。面对网络IO状态繁复的场景，消息驱动IO用处并不大。

### AIO

AIO，即异步IO，Async IO。

前面说到，IO有两个主要步骤，分别是数据从设备到内核空间，和数据从内核空间拷贝到用户空间。`epoll`解决了数据从设备到内核空间过程中大量监听的问题，但是不可否认的是，数据从内核空间到用户空间还是由进程处理的（多路复用，复用的就是这段主要逻辑）。

异步IO则更进一步。设计上，异步IO把数据从内核空间拷贝到用户空间也交给内核处理，在一切都完成之后才通知进程。因此在异步IO中是完全没有阻塞的，这也是“异步”命名的得来。

Linux中还没有完善的异步IO实现。Python中有asyncio的库，待了解。

### 小结

我们常用的IO策略一般就是BIO和IO multiplexing。消息驱动IO使用范围小，AIO目前发展不够完善，没有充分的系统级支持。

实际上考虑I/O过程的两次阻塞， 从内核空间到用户空间的阻塞其实影响不大。AIO能从底层做到数据拷贝到用户空间再通知进程当然很好，但是对于IO本身而言也就一般。

网络IO的一大特点就是第一阶段阻塞时间长，大量连接中同时只会有少数处于激活状态。因此多进程加上进程调度机制（阻塞进程不竞争CPU资源）就可以适应网络IO。

但是网络IO的另一大特点就是并发数巨大，这导致服务器可能没有资源进行多进程的调度。多线程，NIO，IO多路复用都是为了解决这个问题而诞生的。NIO只是让 *socket* 以非阻塞的状态工作，`select`模式把多个 *socket* 并联阻塞的监听交给内核负责，`epoll`则在内核空间直接开辟了`eventpoll`的中间层来减轻多 *socket* 监听的负担。

可以说，IO多路复用的技术演进就是对多路复用的抽象和重新组织。工程上固然没有可以解决所有问题的银弹，但是良好的抽象是可以解决看似矛盾的几个问题的。题外话，设计模式里就是对一些常用良好抽象的总结。

关于多路复用，可以参考这两篇知乎回答：

- [通信和历史角度对多路复用的理解](https://www.zhihu.com/question/32163005/answer/55772739)

- [多路复用的沿革和细节](https://zhuanlan.zhihu.com/p/64138532)

1. 讲一下epoll和select的区别吧（简单说了一下多路复用原理，select要轮询，epoll做了一层抽象，维护了eventpoll，用了红黑树和链表）

2. 什么时候select比epoll好（Never！然后被引导了一下，当有少数长连接的时候用select开销比epoll小一些，分析为什么）