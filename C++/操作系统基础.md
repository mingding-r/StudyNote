# 操作系统基础

## 死锁

在并发环境中，各进程欣慰资源竞争而造成的一种互相等待对方手里的资源，导致各进程都阻塞，都无法继续进行的现象。

### 死锁产生的条件：

1.互斥

2.不可剥夺

3.请求保持

4.循环等待

### 避免死锁的方式：

1.预防死锁：破坏死锁的必要条件

2.避免死锁：如银行家算法（分配资源前先判断一下分配后是否会造成系统处于不安全状态，不会则分配，会则拒绝）

3.死锁的检测和解除

检测：

死锁定理：如果某时刻系统的资源分配图是不可完全简化的，那么此时系统死锁。

解除：

-资源剥夺法；

-撤销进程法

-进程回退法；

另外，系统处于不安全状态未必死锁，但死锁时一定处于不安全状态，系统处于安全状态一定不会死锁。



## 中断

### 中断分类

#### 外部中断

**1.可屏蔽中断**：**通过INTR线向CPU请求的中断**，主要来自外部设备如硬盘，打印机，网卡等。此类中断并不会影响系统运行，可随时处理，甚至不处理，所以名为可屏蔽中断。

**2.不可屏蔽中断**：**通过INTR线向CPU请求的中断**，主要来自外部设备如硬盘，打印机，网卡等。此类中断并不会影响系统运行，可随时处理，甚至不处理，所以名为可屏蔽中断。

#### 内部中断（软中断）

**1.陷阱**：**是一种有意的，预先安排的异常事件**，一般是在编写程序时故意设下的陷阱指令，而后执行到陷阱指令后，CPU将会调用特定程序进行相应的处理，**处理结束后返回到陷阱指令的下一条指令**。

如**系统调用**，程序调试功能等。

**2.故障**：**故障是在引起故障的指令被执行，但还没有执行结束时，CPU检测到的一类的意外事件。**出错时交由故障处理程序处理，**如果能处理修正这个错误，就将控制返回到引起故障的指令即CPU重新执这条指令。如果不能处理就报错。**

常见的故障如 **缺页**

**3.终止**：**执行指令的过程中发生了致命错误，不可修复，程序无法继续运行，只能终止，通常会是一些硬件的错误。**终止处理程序不会将控制返回给原程序，而是直接终止原程序。

![image-20210416163832980](C:\Users\YJDELL\AppData\Roaming\Typora\typora-user-images\image-20210416163832980.png)

## 进程间通信

管道通信、共享内存、消息队列、信号量、信号、socket套接字



### **共享内存：**

最快的一种进程间通信方式，两个不同进程A、B共享内存的意思是，**同一块物理内存被映射到进程A、B各自的进程地址空间。**进程A可以即时看到进程B对共享内存中数据的更新，反之亦然。由于多个进程共享同一块内存区域，必然需要某种同步机制，互斥锁和信号量都可以。

#### **为什么共享内存最快？**

因为进程可以直接读写内存，而不需要任何数据的拷贝。对于像管道和消息队列等通信方式，则需要在内核和用户空间进行四次的数据拷贝，而共享内存则只拷贝两次数据[1]：一次从输入文件到共享内存区，另一次从共享内存区到输出文件。

消息队列和管道基本上都是4次拷贝，而共享内存（mmap, shmget）只有两次。
4次：1，由用户空间的buf中将数据拷贝到内核中。2，内核将数据拷贝到内存中。3，内存到内核。4，内核到用户空间的buf.
2次： 1，用户空间到内存。 2，内存到用户空间。

消息队列和管道都是内核对象，所执行的操作也都是系统调用，而这些数据最终是要存储在内存中执行的。因此不可避免的要经过4次数据的拷贝。但是共享内存不同，当执行mmap或者shmget时，会在内存中开辟空间，然后再将这块空间映射到用户进程的虚拟地址空间中，即返回值为一个指向一个内存地址的指针。当用户使用这个指针时，例如赋值操作，会引起一个从虚拟地址到物理地址的转化，会将数据直接写入对应的物理内存中，省去了拷贝到内核中的过程。当读取数据时，也是类似的过程，因此总共有两次数据拷贝。

#### mmap

在LINUX中我们可以使用mmap用来在进程虚拟内存地址空间中分配地址空间，创建和物理内存的映射关系。

**mmap只是在虚拟内存分配了地址空间，只有在第一次访问虚拟内存的时候才分配物理内存。**

**优点如下：**

1、对文件的读取操作跨过了页缓存，减少了数据的拷贝次数，用内存读写取代I/O读写，提高了文件读取效率。

2、实现了用户空间和内核空间的高效交互方式。两空间的各自修改操作可以直接反映在映射的区域内，从而被对方空间及时捕捉。

3、提供进程间共享内存及相互通信的方式。不管是父子进程还是无亲缘关系的进程，都可以将自身用户空间映射到同一个文件或匿名映射到同一片区域。从而通过各自对映射区域的改动，达到进程间通信和进程间共享的目的。同时，如果进程A和进程B都映射了区域C，当A第一次读取C时通过缺页从磁盘复制文件页到内存中；但当B再读C的相同页面时，虽然也会产生缺页异常，但是不再需要从磁盘中复制文件过来，而可直接使用已经保存在内存中的文件数据。

4、可用于实现高效的大规模数据传输。内存空间不足，是制约大数据操作的一个方面，解决方案往往是借助硬盘空间协助操作，补充内存的不足。但是进一步会造成大量的文件I/O操作，极大影响效率。这个问题可以通过mmap映射很好的解决。换句话说，但凡是需要用磁盘空间代替内存的时候，mmap都可以发挥其功效。

**缺点如下:**

1.文件如果很小，是小于4096字节的，比如10字节，由于内存的最小粒度是页，而进程虚拟地址空间和内存的映射也是以页为单位。虽然被映射的文件只有10字节，但是对应到进程虚拟地址区域的大小需要满足整页大小，因此mmap函数执行后，实际映射到虚拟内存区域的是4096个字节，11~4096的字节部分用零填充。因此如果连续mmap小文件，会浪费内存空间。

1. 对变长文件不适合，文件无法完成拓展，因为mmap到内存的时候，你所能够操作的范围就确定了。

3.如果更新文件的操作很多，会触发大量的脏页回写及由此引发的随机IO上。所以在随机写很多的情况下，mmap方式在效率上不一定会比带缓冲区的一般写快。

### socket

#### socket的相关函数

Bind() ，Listen()，accept ()，send()，receive()，connect()，close();



## 虚拟内存如何映射到物理内存

操作系统为每一个进程维护了一个从虚拟地址到物理地址的映射关系的数据结构，叫页表，页表的内容就是该进程的虚拟地址到物理地址的一个映射。页表中的每一项都记录了这个页的基地址。通过页表，由逻辑地址的高位部分先找到逻辑地址对应的页基地址，再由页基地址偏移一定长度就得到最后的物理地址，偏移的长度由逻辑地址的低位部分决定。一般情况下，这个过程都可以由硬件完成，所以效率还是比较高的。页式内存管理的优点就是比较灵活，内存管理以较小的页为单位，方便内存换入换出和扩充地址空间。

## 内存、缓存、磁盘数据的关系

在计算机的组成结构中，有一个很重要的部分，就是存储器。存储器是用来存储程序和数据的部件，对于计算机来说，有了存储器，才有记忆功能，才能保证正常工作。存储器的种类很多，按其用途可分为主存储器和辅助存储器，主存储器又称内存储器（简称内存），是**CPU能直接寻址的存储空间，它的特点是存取速率快**。辅助存储器又称外存储器（简称外存）。外存通常是磁性介质或光盘，像硬盘，软盘，磁带，CD等，能长期保存信息，并且不依赖于电来保存信息，但是由机械部件带动，速度与CPU相比就显得慢的多。内存指的就是主板上的存储部件，是CPU直接与之沟通，并用其存储数据的部件，存放当前正在使用（即执行中）的数据和程序，它的物理实质就是一组或多组具备数据输入输出和数据存储功能的集成电路，内存只用于暂时存放程序和数据，一旦关闭电源或发生断电，其中的程序和数据就会丢失。 

**内存(Memory)**是计算机的重要部件之一，也称内存储器和主存储器，它用于暂时存放CPU中的运算数据，与硬盘等外部存储器交换的数据。它是外存与CPU进行沟通的桥梁，计算机中所有程序的运行都在内存中进行，内存性能的强弱影响计算机整体发挥的水平。只要计算机开始运行，操作系统就会把需要运算的数据从内存调到CPU中进行运算，当运算完成，CPU将结果传送出来。——动态随机存取存储器（DRAM）

内存一般采用半导体存储单元，包括**随机存储器（RAM）、只读存储器（ROM）和高级缓存（Cache）**。

随机存储器（RAM）可以随机读写数据，但是电源关闭时存储的数据就会丢失；

只读存储器（ROM）：（Read Only Memory）只能读取，不能更改，即使机器断电，数据也不会丢失；



**缓存（cache**），原始意义是指访问速度比一般随机存取存储器（RAM）快的一种高速存储器，通常它不像系统主存那样使用DRAM技术，而使用昂贵但较快速的SRAM技术。缓存的设置是所有现代计算机系统发挥高性能的重要因素之一。——静态随机存取存储器（SRAM）

缓存是介于CPU与内存之间，常用有一级缓存（L1）、二级缓存（L2）、三级缓存（L3）（一般存在于Intel系列）。它的读写速度比内存还快，当CPU在内存中读取或写入数据时，数据会被保存在高级缓冲存储器中，当下次访问该数据时，CPU直接读取高级缓冲存储器，而不是更慢的内存。

提供“缓存”的目的是为了让数据访问的速度适应CPU的处理速度，其基于的原理是内存中“程序执行与数据访问的局域性行为”，即一定程序执行时间和空间内，被访问的代码集中于一部分。为了充分发挥缓存的作用，不仅依靠“暂存刚刚访问过的数据”，还要使用硬件实现的指令预测与数据预取技术——尽可能把将要使用的数据预先从内存中取到缓存里。




主体不同

1、缓存：是指访问速度比一般随机存取存储器（RAM）快的一种高速存储器

2、内存：是计算机中重要的部件之一，它是外存与CPU进行沟通的桥梁。

作用不同

1、缓存：可以进行高速数据交换的存储器，它先于内存与CPU交换数据，因此速率很快。

2、内存：作用是用于暂时存放CPU中的运算数据，以及与硬盘等外部存储器交换的数据。

### Buffer与cache

1、 **Buffer**（缓冲区）是系统两端处理 **速度平衡**（从长时间尺度上看）时使用的。它的引入是为了减小短期内突发I/O的影响，起到 **流量整形**的作用。比如生产者——消费者问题，他们产生和消耗资源的速度大体接近，加一个buffer可以抵消掉资源刚产生/消耗时的突然变化。 
2、 **Cache**（缓存）则是系统两端处理 **速度不匹配**时的一种 **折衷策略**。因为CPU和memory之间的速度差异越来越大，所以人们充分利用数据的局部性（locality）特征，通过使用存储系统分级（memory hierarchy）的策略来减小这种差异带来的影响。 
3、假定以后存储器访问变得跟CPU做计算一样快，cache就可以消失，但是buffer依然存在。比如从网络上下载东西，瞬时速率可能会有较大变化，但从长期来看却是稳定的，这样就能通过引入一个buffer使得OS接收数据的速率更稳定，进一步减少对磁盘的伤害。 
4、TLB（Translation Lookaside Buffer，翻译后备缓冲器）名字起错了，其实它是一个cache.

Buffer的核心作用是用来缓冲，缓和冲击。比如你每秒要写100次硬盘，对系统冲击很大，浪费了大量时间在忙着处理开始写和结束写这两件事嘛。用个buffer暂存起来，变成每10秒写一次硬盘，对系统的冲击就很小，写入效率高了，日子过得爽了。极大缓和了冲击。

Cache的核心作用是加快取用的速度。比如你一个很复杂的计算做完了，下次还要用结果，就把结果放手边一个好拿的地方存着，下次不用再算了。加快了数据取用的速度。

## 同步锁

种类：互斥量和条件变量；读写锁；记录锁/文件锁；信号量。

​		关于互斥锁和自旋锁的实现：

​		实际上锁就是一个变量，通过这个变量的值来判断是不是已加锁，比如可以定义变量lock=1表示未被加锁，lock=0表示已加锁。那么加锁过程可以描述为：判断变量lock是否等于1，如果lock=1，则将lock改为0表示已加锁，然后进入临界区。这时别的线程也想进入临界区，一判断lock=0，那么将不被允许进入临界区，直到拥有锁的线程释放锁lock，即将lock改为1。

​		但是这样会存在几个问题：

1.线程a想要进入临界区，判断lock=1，可以进入临界区，此时在另一个cup核执行的线程b也判断lock=1，也会进入临界区，这样两个线程都会访问临界变量，发生冲突。

2.对于单核也会发生类似错误，比如线程a判断lock=1之后正好发生CPU中断，操作系统调度另一个线程b来执行，恰巧线程b要判断lock是否等于1，由于线程a只是判断lock=1却没来得及将lock置为1，线程b自然会通过lock=1进入临界区，不能保证两者之间的同步。

​		要解决这两个问题，需要保证两点即可：

1.对于lock的判断操作为原子性，要么全部执行，要么全部不执行。

2.一个线程访问lock的时候不允许另一个线程同时访问。

​		要做到这两点，只要禁用中断就可以，而且要禁用所有cup核的中断。这就是解决思路，但是实际并不是这么操作的，因为屏蔽中断会导致系统效率低下，实际上只要保证同一时刻只有一个线程在操作lock就可以了，要做到这一点只要给总线加锁，防止其他核的线程访问到lock。

​     当执行加锁函数lock(lock）时，如果判断到lock已被加锁，那么有两种情况，一种会调用系统函数将当前线程阻塞，还有一种是一直循环处于判断lock状态，如果lock=0接着循环，如果lock=1跳出循环转去临界区，顾名思义这种就是自旋锁。两者各有利弊，已加锁时阻塞掉当前线程让出cup资源可以去执行别的线程，通过减少cup的浪费来提高效率，但是这个过程需要进行上下文切换，保存各寄存器状态需要花费时间，如果每个线程对于锁的占有时间很短，拥有锁之后会很快释放锁，那么自旋锁又更加高效，所以要根据具体情况具体使用。另外值得一提的是自旋锁只能用于多核cup，如果是单核将无限制处于自旋状态。																																																																						

​		前面提到锁本质是一个变量，那么有一个问题，这个变量可以用户程序自己定义吗？实际上是可以的，只要能保证lock(lock)，unlock(lock)函数满足上面提到的要求就可以，但是它会存在一些不便，比如：

- 如果是用户程序定义lock，那么lock变量只能此进程访问，这样无法来对不同进程的线程进行同步，极其不方便。

- 对于阻塞方式的加锁过程，当解锁的时候要判断还有哪些线程申请这个锁，然后将其从等待队列中拿出来放入就绪队列。如果lock是用户程序定义，那么操作系统将无法来判断到底哪些线程申请过这个锁，也将无法将对应的线程激活。

  所以，互斥锁lock变量必须由操作系统来定义，自旋锁可以用户程序定义。下面我们已Linux系统为例，总结一下互斥锁和自旋锁的实现原理。

​		Linux中对于锁的实现都是通过futex系统调用。futex由一块能够被多个进程共享的内存空间组成，保存在用户空间的共享内存中，这样对于futex的操作可以放到用户态来执行而不是在内核态，实际上futex的作用就是减少系统调用的次数来提高系统的性能。

​		下面是互斥锁的实现过程（伪代码）：

```text
//要保证函数体为原子操作

mutex_lock(mutex) {
    lock(bus);    //给总线加锁

    mutex = mutex - 1;
    if(mutex != 0)
        block()
    else
        success

    unlock(bus);  
}


mutex_unlock(mutex) {
    lock(bus);

    mutex = mutex + 1;
    if (mutex != 1) 
        wakeup();
    else
        success
    
   unlock(bus);
}
```

​		在谈自旋锁之前我们先了解一下 CAS操作，CAS(compare and set)是一个定义的函数，它的逻辑功能如下：

```text
bool compare_and_set (int * ptr, int old_val, int new_val) {
    if( *ptr == *old_val ) {
        *ptr = new_val;
        return true;
    }
    return false;
}  //当内存value中的值是old_val则重新赋值为new_val，返回true
```

​		之所以说CAS操作是因为CAS是一个原子性操作，现在几乎所有的cup指令都支持CAS的原子性操作，比如X86下对应的就是CMPXCHG。下面就可以利用CAS来实现自旋锁了。

```text
int owner = 0;   // 1表示自旋锁已分配给某个线程，0表示自旋锁没有分配个任何线程，即处于释放状态

void spin_lock(int * owner ) {
 
    while(!compare_and_set(owner, 0, 1) {
    }

}//需要说明的一点是系统对comare_and_set的内核实现中包含对总线加锁
```

## 并行和并发

### 并发

并发就是多个逻辑线在时间上的重叠。

现代处理器的处理速度非常快，以至于寄存器、内存、外设的速度渐渐跟不上CPU的速度，CPU可以将不同的时间切片留给不同的任务来充分利用资源，在人类眼中产生并行的效果。

有了这样的想法，我们自然就会想要在现代处理器上同时运行多个任务（或者说，这是为了解决多任务而提出的解决方案）。每个运行中的任务都有自己独立的资源，并且在这个时间切片里独享寄存器的资源，将自己的上下文信息加载到寄存器等等。

这种算法被称为时间片轮转，每个任务被分配到一个时间片。Windows中默认的时间切片长度是20ms，Linux/Unix中则是5-800ms。时间片结束的时候，CPU会切换到下一个任务；或者当前任务阻塞了/结束了，也会立即切换到写一个任务的时间片。

在Linux中，不管是进程还是线程，都被当作一个任务结构参与时间片调度。以进程为例，进程有五种状态，分别是`创建` `阻塞` `就绪` `执行` `终止`。其中，只有就绪状态的进程会参与时间片的竞争，这就保证了当一个进程处于阻塞状态的时候，不会消耗CPU资源。

### 并行

所谓并行就是严格意义上的同时处理多个逻辑。

在数字电路设计中，所有的电路都是并行处理（不考虑触发器的时间）。例如，如果有三个加法器电路，则可以使用一次计算的时间得出三个加法的结果。而并发相当于有三个加法同时需要计算，CPU通过时间切片顺序计算了三个加法的结果，在很短的时间内返回，以至于看起来好像同时执行了。

我们都知道CPU就是一组设计过的逻辑电路，那么CPU也可以有多个，这样我们可以同时执行多套指令，得到其结果，这就是多核CPU的原理。多核CPU是多个CPU的组合，可以真正并行地执行不同的逻辑。例如一个四核的CPU，可以有四个线程并行运行，不需要使用时间片轮转调度来实现并发。

超线程则是同步多线程SMT在Intel平台上的实现，通过给一个PC配备两套逻辑单元，配合多发射技术，可以在一个CPU核上模拟多个（两个）逻辑核心的效果，其中每个逻辑核心实际性能都比物理核心差，但是适当条件下两个逻辑核心的总性能会比物理核心强。

## 进程和线程

### 内核态、用户态和虚拟地址空间映射

内核空间就是操作系统管理的地址空间，是所有进程共享的。因为保存了大量重要信息，这个空间是不能被直接访问的，需要切换到内核态。

切换到内核态通常需要一个软中断，或者一个特殊的系统调用。这样的切换会清空寄存器和高速缓存，效率低下。

用户态就是通常的进程运行状态，只能访问自己被分配的空间。为了防止进程修改到其他进程的空间，使用虚拟地址映射。这个技术也被用于虚拟机的空间划分。

虚拟空间和实际物理空间的映射关系在计算机系统中根据“页表”来记录。页表将虚拟空间映射到物理空间，并且在物理空间不够用的时候使用一些算法来继续分配物理空间。例如，覆盖最不可能使用的页（扯呢）；顺序覆盖（不合适吧）；Last_Recent_Used LRU（最常用），LRU cache 可以通过哈希表+双链表来实现。

### 进程

在程序执行的过程中，必须为程序分配一段特定的空间和资源。在程序执行期间，其栈空间肯定不能被其他手段修改，对于PC EBP ESP 的使用也是独占的，等等。

因此，一个运行中的程序和这段代码本身是有区别的，我们将运行中的程序称为`Process`进程。

#### 进程和程序的区别

我们知道现代CPU中会有多个任务并发执行，这个时候`Program`就不能很好地描述这些在执行中的任务了。

一个写好的`Program`存储在程序段中可以不被执行，当然也可以被同时执行好几次。只要分配了不同的资源，同一个`Program`作为多个任务启动也可以并行不悖。因此，我们使用进程`Process`来描述一个运行中的程序。

对于一个进程而言，其关心的就是自己的代码段信息，以及执行过程中分配的文件资源、栈空间，以及在它的时间片内独享的上下文信息。

通常而言，进程是一个程序运行状态的表述，是多任务中的一个任务。

#### 进程的切换

进程有 创建-阻塞-就绪-执行-终止 五种状态。在进程切换的时候，如果遇到IO请求失败等，可能将进程挂起到阻塞状态；如果是遇到时间切片结束了，则会进入就绪状态。如果程序提前结束或者阻塞，也会立即切换执行进程。

进程切换的时候要切换页表，这将使得页表缓存TLC失效，影响接下来的内存操作速度；进程切换还要重新加载PCB（Process Control Block）进程控制块，控制块里有进程相关的信息，尤其是PC和其他寄存器的值（即上下文信息）。

因此，进程的创建本身会消耗一定的资源，例如预分配的栈空间。进程的切换也需要消耗一定的CPU资源，并降低一段时间内的CPU性能。

### 线程

从概念上看，线程是进程的附属单元。系统为进程分配资源，线程共享进程的资源，是进程内逻辑执行的基本单元。一个进程至少有一个线程，这个线程被称作主线程。

#### 进程和线程

最早的操作系统里是没有线程的，进程就是调度的基本单位。但是因为进程切换开销太大，所以在80年代提出了线程的概念。进程是线程的容器，一个进程里可以有多个线程。有一个很生动形象的比喻是：进程是火车，线程是车厢。

从理论上来说，线程是进程内部的具体工作流，外部是不知道进程内部有哪些线程的。

在Linux系统中，进程和线程是一种东西，都是Task。线程是轻量级的进程。创建一个线程的时间开销要比创建一个进程大一些，但是空间开销要小很多。一个进程创建需要MB级别的内存分配，一个线程的创建可能只需要几百KB级别的内存分配。这意味着在需要大量进程/线程协作的场景下，使用线程可以让系统支持更多的多任务。

在Linux系统中，线程现在是实际参与调度分配的最小单位。进程创建线程之后，通过系统调用让线程参与计算资源的分配。不然，一个多线程的任务可能只能共享父进程的计算资源（比如一个核心）。

#### 线程间的资源共享

进程之间是无法共享资源的，他们拥有不同的虚拟内存映射页表，无法直接修改彼此的值（只能说这是进程的特点，而不一定是缺点。不能共享内存反过来说就意味着安全）。

线程之间可以直接共享信息，因为即使线程有不同的栈帧空间，但是线程只要是共享一套地址，理论上就可以直接修改对方的值。当然，一般不会进行这种危险操作，而是一开始就设计所有线程都能访问的变量。多线程同时操作（如果能）一个变量就会引起线程安全问题。工程上没有银弹，想要解决通信开销就要引入线程锁的问题。

#### 线程的切换

提出线程就是为了减少进程切换的开销。

我们知道，切换到一个新的`Task`一定会导致CPU的PC、ESP、EBP等资源加载新的信息。因此，对于线程，也有一个线程控制块PCB来记录这些信息。

线程的切换不需要重新加载页表信息，相当于减少了资源切换的开销；线程之间通信也不需要使用socket通信等方式。这意味着理论上使用线程会比进程更节约资源。

当然，在Linux系统中，因为线程实际上是轻量级进程，在切换进程/线程的时候都是将他们当作`task_struct`信息加载上下文。因此，线程切换并不会比进程切换成本更低。线程相对于进程的优势主要是不需要预分配大量的栈空间资源，可以在同样配置的机器上创建更多的线程。

## 总结

最早的系统中只有进程的概念。

程序是一段有序指令的总称，而程序的运行需要用到CPU的寄存器资源，以及特定的内存资源。为了保证内存资源的使用，操作系统发展出了虚拟内存和分页表来确保程序运行时不会相互影响，使用LRU技术管理内存映射。

既然程序运行有特定的上下文和资源，运行的程序和程序概念就不一样了。一个运行中的程序被称为进程，操作系统为进程分配资源并进行多进程的调度来实现并发执行任务。

并发执行并不是并行执行，并发只是不同逻辑在时间上交替执行，看起来像是一起执行；并行则是不同逻辑同时执行。多核CPU从物理电路上支持并行执行多任务。

进程和线程在逻辑上是包含的关系。有了线程的定义之后，进程就成为了程序和分配的资源的统称，而程序实际的运行调度对象就变成了线程。

严格来说，线程应该是隐藏在进程里的，类似于协程。但是在实现上，Linux把线程视为轻量级进程。线程的切换除了不需要加载分页表，寄存器上下文、内核中的`task_struct`都要重新加载。因此，线程的切换开销也很大，但是其他资源占用更低。

在高并发的情况下，线程可以让机器承载更高的并发量。线程之间的资源共享不需要通过通信机制，只要做好多线程同步即可。



## 生产者消费者模型

### 说一下进程和线程 它们的区别

### 进程通信方式

### 共享内存是怎么实现的？（mmap?)

### fork()之后两进程之间修改后结果 取地址后结果

1.什么是用户态内核态

2.进程线程的使用选择（场景）

3.线程锁的类型

4.锁锁的什么？

5.给我一个必死锁的场景，最简单的那种



2.条件变量和信号量的区别

3.预防死锁的方法  及银行家算法中如何判断不安全